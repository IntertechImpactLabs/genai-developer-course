{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo - Instructor Version\n",
    "\n",
    "## Providing Domain-Specific Context with Multiple LLMs\n",
    "\n",
    "This notebook demonstrates Retrieval-Augmented Generation (RAG) using both OpenAI and Anthropic models.\n",
    "\n",
    "### Prerequisites\n",
    "- Set `OPENAI_API_KEY` environment variable\n",
    "- Set `ANTHROPIC_API_KEY` environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Import AI libraries\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized\")\n",
    "print(\"‚úÖ Anthropic client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Product Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock product catalog\n",
    "PRODUCTS = [\n",
    "    {\n",
    "        \"name\": \"LicketySplit Pro\",\n",
    "        \"description\": \"Lightning-fast in-memory caching solution with sub-millisecond latency\",\n",
    "        \"features\": [\"10GB capacity\", \"LRU eviction\", \"distributed mode\", \"Redis compatible\"],\n",
    "        \"keywords\": [\"speed\", \"fast\", \"performance\", \"cache\", \"memory\", \"quick\", \"turbo\"],\n",
    "        \"price\": \"$99/month\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Vault-Tec Enterprise\",\n",
    "        \"description\": \"Military-grade encryption for sensitive data protection\",\n",
    "        \"features\": [\"AES-256 encryption\", \"biometric auth\", \"SOC2 compliant\", \"key rotation\"],\n",
    "        \"keywords\": [\"security\", \"encryption\", \"protection\", \"vault\", \"safe\", \"secure\", \"privacy\"],\n",
    "        \"price\": \"$199/month\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DreamCloud Manager\",\n",
    "        \"description\": \"Real-time data synchronization across cloud platforms\",\n",
    "        \"features\": [\"Multi-cloud support\", \"version control\", \"1TB storage\", \"automatic backups\"],\n",
    "        \"keywords\": [\"sync\", \"cloud\", \"backup\", \"storage\", \"synchronization\", \"replication\"],\n",
    "        \"price\": \"$149/month\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DiggityDog Analytics\",\n",
    "        \"description\": \"Stream processing and real-time analytics platform\",\n",
    "        \"features\": [\"Apache Kafka integration\", \"ML pipelines\", \"custom dashboards\", \"alerting\"],\n",
    "        \"keywords\": [\"analytics\", \"data\", \"streaming\", \"metrics\", \"insights\", \"dashboard\"],\n",
    "        \"price\": \"$299/month\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(PRODUCTS)} products:\")\n",
    "for p in PRODUCTS:\n",
    "    print(f\"  - {p['name']}: {p['description'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Knowledge Gap Problem\n",
    "\n",
    "Let's demonstrate what happens when LLMs don't have access to your specific product information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_without_context(question: str, model: str = \"openai\"):\n",
    "    \"\"\"Ask LLM without any context - it lacks domain-specific knowledge!\"\"\"\n",
    "    \n",
    "    if model == \"gpt-3.5\":\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    elif model == \"gpt-4o\":\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    elif model == \"claude-haiku\":\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.content[0].text\n",
    "    \n",
    "    elif model == \"claude-sonnet\":\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "# Test with multiple models\n",
    "question = \"What features does LicketySplit have? What's the pricing?\"\n",
    "\n",
    "display(Markdown(\"## Testing Knowledge Gap Across Different Models\"))\n",
    "\n",
    "# Test older models\n",
    "display(Markdown(\"### ü§ñ GPT-3.5 Turbo (Older Model):\"))\n",
    "gpt35_response = ask_llm_without_context(question, \"gpt-3.5\")\n",
    "display(Markdown(f\"\\n{gpt35_response}\\n\"))\n",
    "\n",
    "display(Markdown(\"### ü§ñ GPT-4o (Newer OpenAI Model):\"))\n",
    "gpt4o_response = ask_llm_without_context(question, \"gpt-4o\")\n",
    "display(Markdown(f\"\\n{gpt4o_response}\\n\"))\n",
    "\n",
    "display(Markdown(\"### ü§ñ Claude Haiku 3 (Older Model):\"))\n",
    "haiku_response = ask_llm_without_context(question, \"claude-haiku\")\n",
    "display(Markdown(f\"\\n{haiku_response}\\n\"))\n",
    "\n",
    "display(Markdown(\"### ü§ñ Claude Sonnet 4 (Newer Anthropic Model):\"))\n",
    "sonnet_response = ask_llm_without_context(question, \"claude-sonnet\")\n",
    "display(Markdown(f\"\\n{sonnet_response}\\n\"))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "‚ö†Ô∏è **Key Observation:** Models lack knowledge of your internal products and proprietary information. RAG provides this domain-specific context.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Keyword-Based RAG\n",
    "\n",
    "Now let's implement simple keyword search to retrieve real product information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the keyword search results show\n",
    "\n",
    "This section demonstrates **Retrieval-Augmented Generation (RAG)** using a simple keyword search for retrieval:\n",
    "\n",
    "1. **Retrieval:** Find relevant products using a basic full-text match (your query tokens vs each product‚Äôs name, description, features, and keywords).\n",
    "2. **Context Construction:** Build a context block from the top results.\n",
    "3. **Generation:** Pass the context to the LLM to answer the question, grounded in real data.\n",
    "\n",
    "- The list is ordered by how many tokens overlap (more overlap = more relevant).\n",
    "- This is the simplest form of RAG: deterministic, offline, and easy to explain.\n",
    "- The next section will show how semantic search can improve retrieval for less obvious matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword search utilities used in this section\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> set:\n",
    "    \"\"\"Lowercase alphanumeric tokenization returning a set of unique tokens.\"\"\"\n",
    "    if not text:\n",
    "        return set()\n",
    "    return set(re.findall(r\"[a-z0-9]+\", text.lower()))\n",
    "\n",
    "\n",
    "def search_products_keyword(query: str, products: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Simple full-text overlap across name, description, features, and keywords.\"\"\"\n",
    "    q_tokens = _tokenize(query)\n",
    "    scored = []\n",
    "\n",
    "    for prod in products:\n",
    "        name = prod.get(\"name\", \"\")\n",
    "        description = prod.get(\"description\", \"\")\n",
    "        features = \", \".join(prod.get(\"features\", []))\n",
    "        keywords = \" \".join(prod.get(\"keywords\", []))\n",
    "        combined = f\"{name} {description} {features} {keywords}\"\n",
    "        p_tokens = _tokenize(combined)\n",
    "        overlap = len(q_tokens & p_tokens)\n",
    "        if overlap > 0:\n",
    "            scored.append((prod, overlap))\n",
    "\n",
    "    scored.sort(key=lambda x: (-x[1], x[0].get(\"name\", \"\")))\n",
    "    return [p for p, _ in scored[:top_k]]\n",
    "\n",
    "\n",
    "def create_context(products: List[Dict]) -> str:\n",
    "    \"\"\"Format selected products into a readable context block, one field per line.\"\"\"\n",
    "    lines: List[str] = []\n",
    "    for p in products:\n",
    "        lines.append(f\"Name: {p.get('name', '')}\")\n",
    "        lines.append(f\"Description: {p.get('description', '')}\")\n",
    "        feats = \", \".join(p.get(\"features\", []))\n",
    "        if feats:\n",
    "            lines.append(f\"Features: {feats}\")\n",
    "        price = p.get(\"price\")\n",
    "        if price:\n",
    "            lines.append(f\"Price: {price}\")\n",
    "        lines.append(\"\")  # Blank line between products\n",
    "    return \"\\n\".join(lines).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Keyword Search\n",
    "for q in [\"fast cache performance\", \"security encryption\", \"cloud backup\"]:\n",
    "    q_tokens = _tokenize(q)\n",
    "    scored = []\n",
    "    for prod in PRODUCTS:\n",
    "        name = prod.get('name', '')\n",
    "        description = prod.get('description', '')\n",
    "        features = \", \".join(prod.get('features', []))\n",
    "        keywords = \" \".join(prod.get('keywords', []))\n",
    "        combined = f\"{name} {description} {features} {keywords}\"\n",
    "        p_tokens = _tokenize(combined)\n",
    "        overlap = len(q_tokens & p_tokens)\n",
    "        if overlap > 0:\n",
    "            scored.append((prod['name'], overlap))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    display(Markdown(f\"**Query:** `{q}`\"))\n",
    "    if scored:\n",
    "        lines = [f\"- {name} (overlap: {ov})\" for name, ov in scored]\n",
    "        display(Markdown(\"Results (highest overlap first):\\n\" + \"\\n\".join(lines)))\n",
    "    else:\n",
    "        display(Markdown(\"No matches\"))\n",
    "    display(HTML(\"<hr style='margin: 10px 0;'>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_augmented_prompt(question: str, products: List[Dict]):\n",
    "    \"\"\"Create an augmented prompt from keyword search results and return it with results.\"\"\"\n",
    "    search_results = search_products_keyword(question, products)\n",
    "    context = create_context(search_results) if search_results else \"No product information available.\"\n",
    "\n",
    "    prompt = f\"\"\"Based on the following product information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based only on the provided information. If the information doesn't answer the question, say so.\"\"\"\n",
    "    return prompt, search_results\n",
    "\n",
    "\n",
    "def ask_llm(prompt: str, model: str = \"openai\"):\n",
    "    \"\"\"Call the selected LLM with the already-constructed prompt.\"\"\"\n",
    "    if model == \"openai\":\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-5-mini-2025-08-07\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer based only on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    elif model == \"anthropic\":\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "# Demo: generate the augmented prompt first, print it, then ask the LLMs\n",
    "question = \"What features does LicketySplit have? What's the pricing?\"\n",
    "\n",
    "display(Markdown(\"## With RAG - Domain-Specific Responses\"))\n",
    "\n",
    "augmented_prompt, _results = generate_augmented_prompt(question, PRODUCTS)\n",
    "\n",
    "# Print the augmented prompt as a Markdown code block for perfect formatting\n",
    "prompt_md = f\"\"\"### Augmented Prompt\\n\\n```text\\n{augmented_prompt}\\n```\"\"\"\n",
    "display(Markdown(prompt_md))\n",
    "\n",
    "display(Markdown(\"### ‚úÖ GPT-5 mini with RAG:\"))\n",
    "gpt_rag = ask_llm(augmented_prompt, \"openai\")\n",
    "display(Markdown(f\"\\n{gpt_rag}\\n\"))\n",
    "\n",
    "display(Markdown(\"### ‚úÖ Claude with RAG:\"))\n",
    "claude_rag = ask_llm(augmented_prompt, \"anthropic\")\n",
    "display(Markdown(f\"\\n{claude_rag}\\n\"))\n",
    "\n",
    "display(Markdown(\"‚úÖ **These responses are accurate - based on your actual product data!**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Semantic Search (Advanced)\n",
    "\n",
    "This section demonstrates **RAG with semantic search**:\n",
    "\n",
    "1. **Retrieval:** Find relevant products using vector similarity (embeddings) to capture meaning, not just keywords.\n",
    "2. **Context Construction:** Build a context block from the most semantically similar products.\n",
    "3. **Generation:** Pass the context to the LLM for a grounded answer.\n",
    "\n",
    "- Semantic search can find related concepts even without exact keyword matches.\n",
    "- This approach is more robust for natural language queries, but requires API access and is more computationally expensive.\n",
    "- Compare the results to the keyword search to see the difference in retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\"):\n",
    "    \"\"\"Get embedding from OpenAI\"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Generate embeddings for all products\n",
    "print(\"Generating embeddings for products...\")\n",
    "for product in PRODUCTS:\n",
    "    text = f\"{product['name']} {product['description']}\"\n",
    "    product['embedding'] = get_embedding(text)\n",
    "print(\"‚úÖ Embeddings generated\")\n",
    "\n",
    "def semantic_search(query: str, products: List[Dict], top_k: int = 2) -> List[Dict]:\n",
    "    \"\"\"Search using semantic similarity\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    similarities = []\n",
    "    for product in products:\n",
    "        sim = cosine_similarity(query_embedding, product['embedding'])\n",
    "        similarities.append((product, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [s[0] for s in similarities[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare keyword vs semantic search\n",
    "semantic_queries = [\n",
    "    (\"high-speed data access\", \"LicketySplit Pro\"),       # Fast cache\n",
    "    (\"protect sensitive information\", \"Vault-Tec Enterprise\"),  # Security\n",
    "    (\"cloud backup\", \"DreamCloud Manager\"),               # Backup/cloud\n",
    "    (\"real-time insights\", \"DiggityDog Analytics\")        # Analytics\n",
    "]\n",
    "\n",
    "display(Markdown(\"## Keyword vs Semantic Search Comparison\"))\n",
    "\n",
    "for query, expected in semantic_queries:\n",
    "    display(Markdown(f\"### Query: `{query}`\"))\n",
    "    display(Markdown(f\"**Expected:** {expected}\"))\n",
    "    \n",
    "    # Keyword search (simple full-text overlap)\n",
    "    keyword_results = search_products_keyword(query, PRODUCTS, top_k=3)\n",
    "    keyword_names = [r['name'] for r in keyword_results] if keyword_results else [\"No matches\"]\n",
    "    \n",
    "    # Semantic search (requires embeddings/API)\n",
    "    semantic_results = semantic_search(query, PRODUCTS, top_k=3)\n",
    "    semantic_names = [s['name'] for s in semantic_results] if semantic_results else [\"No matches\"]\n",
    "    \n",
    "    # Display results as a Markdown table (no code block, no leading spaces)\n",
    "    table_md = (\n",
    "        \"| Method | Top 3 Results | Status |\\n\"\n",
    "        \"|--------|----------------|--------|\\n\"\n",
    "        f\"| Keyword Search | {', '.join(keyword_names)} | {'‚úÖ' if expected in keyword_names else '‚ùå'} |\\n\"\n",
    "        f\"| Semantic Search | {', '.join(semantic_names)} | {'‚úÖ Found via meaning!' if expected in semantic_names else '‚ùå'} |\"\n",
    "    )\n",
    "    display(Markdown(table_md))\n",
    "    \n",
    "    display(HTML(\"<hr style='margin: 20px 0;'>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparing Model Performance with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_with_semantic_rag(question: str, products: List[Dict]):\n",
    "    \"\"\"Compare different models using semantic RAG\"\"\"\n",
    "    \n",
    "    # Use semantic search for better results\n",
    "    search_results = semantic_search(question, products, top_k=2)\n",
    "    context = create_context(search_results)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following product information:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a detailed answer based only on the provided information.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test GPT-3.5\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer based only on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        results['GPT-3.5 Turbo'] = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        results['GPT-3.5 Turbo'] = f\"Error: {e}\"\n",
    "    \n",
    "    # Test GPT-4o\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer based only on the provided context.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        results['GPT-4o'] = response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        results['GPT-4o'] = f\"Error: {e}\"\n",
    "    \n",
    "    # Test Claude Haiku\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        results['Claude Haiku 3'] = response.content[0].text\n",
    "    except Exception as e:\n",
    "        results['Claude Haiku 3'] = f\"Error: {e}\"\n",
    "    \n",
    "    # Test Claude Sonnet\n",
    "    try:\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        results['Claude Sonnet 4'] = response.content[0].text\n",
    "    except Exception as e:\n",
    "        results['Claude Sonnet 4'] = f\"Error: {e}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models\n",
    "question = \"What's the best solution for high-performance data caching with Redis compatibility?\"\n",
    "\n",
    "display(Markdown(f\"## Model Comparison with Semantic RAG\\n\\n**Question:** {question}\"))\n",
    "display(Markdown(\"*Now with RAG, all models provide accurate information based on real data*\\n\"))\n",
    "\n",
    "model_responses = compare_models_with_semantic_rag(question, PRODUCTS)\n",
    "\n",
    "for model, response in model_responses.items():\n",
    "    display(Markdown(f\"### {model}:\"))\n",
    "    display(Markdown(f\"\\n{response}\\n\"))\n",
    "    display(HTML(\"<hr style='margin: 20px 0;'>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
