{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo - Instructor Version\n",
    "\n",
    "## From Hallucination to Accuracy\n",
    "\n",
    "This notebook demonstrates the progression from AI hallucination to accurate responses using Retrieval-Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Try to import OpenAI for semantic search demo\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    import numpy as np\n",
    "    OPENAI_AVAILABLE = True\n",
    "    print(\"‚úÖ OpenAI available - semantic search demo will use real embeddings\")\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  OpenAI not installed - will use mock embeddings for demo\")\n",
    "\n",
    "# Load products from JSON\n",
    "with open('mock_products.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    PRODUCTS = data['products']\n",
    "\n",
    "print(f\"\\nLoaded {len(PRODUCTS)} products:\")\n",
    "for p in PRODUCTS:\n",
    "    print(f\"  - {p['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Hallucination Problem\n",
    "\n",
    "Let's see what happens when AI doesn't have access to real product information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ai_without_rag(query: str) -> str:\n",
    "    \"\"\"Simulate AI response without any real data\"\"\"\n",
    "    \n",
    "    if \"TurboCache\" in query:\n",
    "        return \"\"\"\n",
    "Based on the name, TurboCache Pro likely includes:\n",
    "- Advanced ML-based caching algorithms\n",
    "- Automatic scaling to 100TB\n",
    "- Built-in blockchain verification  \n",
    "- Quantum-resistant encryption\n",
    "- GraphQL API support\n",
    "- Free tier with 5GB storage\n",
    "\n",
    "Price: Probably starts at $49/month\n",
    "\n",
    "‚ö†Ô∏è Note: These features are COMPLETELY MADE UP!\n",
    "The AI is hallucinating plausible-sounding features.\n",
    "\"\"\"\n",
    "    return \"I don't have information about that product.\"\n",
    "\n",
    "# Demonstrate hallucination\n",
    "query = \"Tell me about TurboCache Pro features\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AI Response WITHOUT Real Data:\")\n",
    "print(\"=\"*50)\n",
    "print(simulate_ai_without_rag(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Keyword-Based RAG\n",
    "\n",
    "Now let's implement simple keyword search to retrieve real product information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_products_keyword(query: str, products: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Simple keyword search\"\"\"\n",
    "    query_words = query.lower().split()\n",
    "    matches = []\n",
    "    \n",
    "    for product in products:\n",
    "        match_count = sum(\n",
    "            1 for word in query_words \n",
    "            if word in product['keywords']\n",
    "        )\n",
    "        \n",
    "        if match_count > 0:\n",
    "            matches.append({\n",
    "                'product': product,\n",
    "                'relevance': match_count\n",
    "            })\n",
    "    \n",
    "    matches.sort(key=lambda x: x['relevance'], reverse=True)\n",
    "    return [m['product'] for m in matches]\n",
    "\n",
    "# Test keyword search\n",
    "test_queries = [\n",
    "    \"fast cache performance\",\n",
    "    \"security encryption\",\n",
    "    \"cloud backup\",\n",
    "    \"quantum computing\"  # Should find nothing\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search_products_keyword(query, PRODUCTS)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Results: {[r['name'] for r in results] if results else 'No matches'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_response(query: str, products: List[Dict]) -> str:\n",
    "    \"\"\"Generate response using RAG\"\"\"\n",
    "    \n",
    "    # Search for relevant products\n",
    "    results = search_products_keyword(query, products)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No matching products found in our catalog.\"\n",
    "    \n",
    "    # Use first result for response\n",
    "    product = results[0]\n",
    "    \n",
    "    response = f\"\"\"\n",
    "Based on our product catalog:\n",
    "\n",
    "Product: {product['name']}\n",
    "Description: {product['description']}\n",
    "\n",
    "Features:\n",
    "{chr(10).join(f'- {f}' for f in product['features'])}\n",
    "\n",
    "Price: {product['price']}\n",
    "\n",
    "‚úÖ This information is ACCURATE - retrieved from real data!\n",
    "\"\"\"\n",
    "    return response\n",
    "\n",
    "# Compare with/without RAG\n",
    "query = \"tell me about turbocache pro\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AI Response WITH RAG:\")\n",
    "print(\"=\"*50)\n",
    "print(create_rag_response(query, PRODUCTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Semantic Search (Advanced)\n",
    "\n",
    "Semantic search finds related concepts even without exact keyword matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPENAI_AVAILABLE:\n",
    "    # Real embeddings demo\n",
    "    client = OpenAI()\n",
    "    \n",
    "    def get_embedding(text: str):\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    def cosine_similarity(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    # Generate embeddings for products\n",
    "    print(\"Generating embeddings for products...\")\n",
    "    for product in PRODUCTS:\n",
    "        text = f\"{product['name']} {product['description']}\"\n",
    "        product['embedding'] = get_embedding(text)\n",
    "    print(\"‚úÖ Embeddings generated\")\n",
    "    \n",
    "    def semantic_search(query: str, products: List[Dict], top_k: int = 2) -> List[Dict]:\n",
    "        \"\"\"Search using semantic similarity\"\"\"\n",
    "        query_embedding = get_embedding(query)\n",
    "        \n",
    "        similarities = []\n",
    "        for product in products:\n",
    "            sim = cosine_similarity(query_embedding, product['embedding'])\n",
    "            similarities.append((product, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [s[0] for s in similarities[:top_k]]\n",
    "    \n",
    "else:\n",
    "    # Mock semantic search\n",
    "    def semantic_search(query: str, products: List[Dict], top_k: int = 2) -> List[Dict]:\n",
    "        \"\"\"Mock semantic search for demo\"\"\"\n",
    "        # Simulate semantic understanding\n",
    "        mock_mappings = {\n",
    "            \"high-speed\": \"TurboCache Pro\",\n",
    "            \"protect\": \"SecureVault Enterprise\",\n",
    "            \"team\": \"CloudSync Manager\",\n",
    "            \"insights\": \"DataFlow Analytics\"\n",
    "        }\n",
    "        \n",
    "        for key, product_name in mock_mappings.items():\n",
    "            if key in query.lower():\n",
    "                return [p for p in products if p['name'] == product_name]\n",
    "        return []\n",
    "\n",
    "# Compare keyword vs semantic search\n",
    "semantic_queries = [\n",
    "    \"high-speed data access\",  # No \"fast\" keyword\n",
    "    \"protect sensitive information\",  # No \"security\" keyword\n",
    "    \"team collaboration tools\"  # Inferred meaning\n",
    "]\n",
    "\n",
    "for query in semantic_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Keyword search\n",
    "    keyword_results = search_products_keyword(query, PRODUCTS)\n",
    "    print(f\"Keyword search: {[r['name'] for r in keyword_results] if keyword_results else '‚ùå No matches'}\")\n",
    "    \n",
    "    # Semantic search\n",
    "    semantic_results = semantic_search(query, PRODUCTS)\n",
    "    print(f\"Semantic search: {[r['name'] for r in semantic_results] if semantic_results else 'No matches'}\")\n",
    "    \n",
    "    if semantic_results and not keyword_results:\n",
    "        print(\"‚úÖ Semantic search found meaning without exact keywords!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cost & Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost comparison\n",
    "print(\"RAG Cost Analysis (per 1000 queries):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cost_data = [\n",
    "    (\"Keyword Search\", \"<$0.01\", \"Instant\", \"70%\"),\n",
    "    (\"Semantic Search\", \"~$0.05\", \"100ms\", \"90%\"),\n",
    "    (\"GPT-4 without RAG\", \"~$0.30\", \"1-2s\", \"Variable/Hallucination\"),\n",
    "    (\"Fine-tuning\", \"$500+ upfront\", \"Instant\", \"95%\")\n",
    "]\n",
    "\n",
    "print(f\"{'Method':<20} {'Cost':<15} {'Speed':<10} {'Accuracy'}\")\n",
    "print(\"-\" * 60)\n",
    "for method, cost, speed, accuracy in cost_data:\n",
    "    print(f\"{method:<20} {cost:<15} {speed:<10} {accuracy}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"- RAG is 6x cheaper than raw GPT-4 calls\")\n",
    "print(\"- Eliminates hallucination risk\")\n",
    "print(\"- Start with keyword search, upgrade to semantic as needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Demonstrate text chunking for large documents\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example of chunking\n",
    "long_doc = \"This is a very long document about product features. \" * 200\n",
    "chunks = chunk_text(long_doc, chunk_size=50, overlap=10)\n",
    "\n",
    "print(f\"Document chunking example:\")\n",
    "print(f\"- Original: {len(long_doc.split())} words\")\n",
    "print(f\"- Chunks: {len(chunks)} chunks\")\n",
    "print(f\"- Chunk size: ~50 words with 10 word overlap\")\n",
    "print(f\"\\nFirst chunk preview:\")\n",
    "print(chunks[0][:100] + \"...\")\n",
    "\n",
    "print(\"\\nüìã Production Checklist:\")\n",
    "checklist = [\n",
    "    \"‚úÖ Handle 'no results' gracefully\",\n",
    "    \"‚úÖ Set similarity thresholds\",\n",
    "    \"‚úÖ Implement caching for embeddings\",\n",
    "    \"‚úÖ Monitor retrieval quality\",\n",
    "    \"‚úÖ Add user feedback loop\",\n",
    "    \"‚úÖ Version your knowledge base\"\n",
    "]\n",
    "for item in checklist:\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RAG eliminates hallucinations** by grounding responses in real data\n",
    "2. **Start simple** with keyword search - often sufficient\n",
    "3. **Semantic search** adds intelligence but costs more\n",
    "4. **The pattern**: Search ‚Üí Augment ‚Üí Generate\n",
    "5. **Production needs**: Error handling, monitoring, feedback\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|---------------|\n",
    "| Product catalog | Keyword RAG |\n",
    "| Technical documentation | Semantic RAG |\n",
    "| Frequently changing data | RAG (not fine-tuning) |\n",
    "| Domain-specific language | Fine-tuning |\n",
    "| General knowledge | No RAG needed |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}